{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dropout\n",
    "import re\n",
    "import nltk\n",
    "import chart_studio\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "from bs4 import BeautifulSoup\n",
    "import plotly.graph_objects as go\n",
    "import chart_studio.plotly as py\n",
    "import cufflinks\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "import plotly.figure_factory as ff\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "from chart_studio.plotly import iplot\n",
    "cufflinks.go_offline()\n",
    "cufflinks.set_config_file(world_readable=True, theme='pearl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 50000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 250\n",
    "\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lstm_issue():\n",
    "\n",
    "    def __init__(self, data_location, data_location_ori = 'data/hadoop/HADOOP.csv', aug_mul = 2):\n",
    "        self.data_ori = pd.read_csv(data_location_ori) # 원본 데이터\n",
    "        self.aug_mul = aug_mul\n",
    "        self.df = pd.read_csv(data_location, encoding='cp949')\n",
    "        print(self.df.info())\n",
    "\n",
    "        print(self.df.component.value_counts())\n",
    "\n",
    "        \n",
    "        self.data_ori['text'] = list(self.data_ori.title + \" \" + self.data_ori.description)\n",
    "\n",
    "        for x in range(len(self.df.component)):\n",
    "            self.df.component[x] = self.df.component[x].split(',')[0]\n",
    "\n",
    "        self.Y = pd.get_dummies(self.df[set(self.df.component)])\n",
    "        print(self.df.component.value_counts())\n",
    "\n",
    "    def print_plot(self, index):\n",
    "        example = self.df[self.df.index == index][['text', 'component']].values[0]\n",
    "        if len(example) > 0:\n",
    "            print(example[0])\n",
    "            print('component:', example[1])\n",
    "\n",
    "    def preprocessing(self):\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "        self.df = self.df.astype(str)\n",
    "    \n",
    "    def clean_text(self):\n",
    "        '''self.df['text'] = self.df['text'].apply(_clean_text)\n",
    "        self.df['text'] = self.df['text'].str.replace('\\d+', '')'''\n",
    "        refined_data = []\n",
    "        for item in self.df['text']:\n",
    "            #1. Remove \\r \n",
    "            current_desc = item.replace('\\r', ' ')    \n",
    "            #2. Remove URLs\n",
    "            current_desc = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', current_desc)    \n",
    "            #4. Remove hex code\n",
    "            current_desc = re.sub(r'(\\w+)0x\\w+', '', current_desc) \n",
    "            #5. Change to lower case\n",
    "            current_desc = current_desc.lower()   \n",
    "            #6. Tokenize\n",
    "            #current_desc_tokens = tokenizer(current_desc, add_special_tokens= True)\n",
    "            #7. Strip trailing punctuation marks    \n",
    "            #current_desc_filter = [word.strip(string.punctuation) for word in current_desc_tokens]     \n",
    "            #8. Join the lists\n",
    "            #current_data = current_desc_filter\n",
    "            #current_data = list(filter(None, current_data))\n",
    "            refined_data.append(current_desc)\n",
    "        self.df['text'] = refined_data\n",
    "        self.df['text'] = self.df['text'].str.replace('\\d+', '')\n",
    "    \n",
    "    def tokenize_df(self):\n",
    "        '''# The maximum number of words to be used. (most frequent)\n",
    "        MAX_NB_WORDS = 50000\n",
    "        # Max number of words in each complaint.\n",
    "        MAX_SEQUENCE_LENGTH = 250\n",
    "        # This is fixed.\n",
    "        EMBEDDING_DIM = 100'''\n",
    "        self.tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "        self.tokenizer.fit_on_texts(self.df['text'].values)\n",
    "        word_index = self.tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "    # 불러온 정제된 데이터 one hot을 str에서 list로 바꾸는 작업\n",
    "    def labels_to_int(self):\n",
    "        x, y = train_test_split(self.data_ori, test_size = 0.2, random_state=42)\n",
    "        self.X_ori = x.text\n",
    "        self.Y_ori = y.component\n",
    "\n",
    "        #self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X_ori, self.Y_ori, test_size = 0.20, random_state = 21)\n",
    "\n",
    "        # 증강데이터의 train_data에서 test index부분 제거\n",
    "        y_index = list(y.index)\n",
    "        #test_index_list = list(self.test_data.index)\n",
    "        test_index =[]\n",
    "        for aug_num in range(self.aug_mul):\n",
    "            iidf2 = [i + 6152* aug_num for i in y_index]\n",
    "            test_index = test_index + iidf2\n",
    "\n",
    "        # labeling y\n",
    "        #self.Y = pd.get_dummies(self.df['component'])\n",
    "        df_compset = list(set(self.df.component.values))\n",
    "        #self.Y = pd.get_dummies(self.df[set(self.df.component)])\n",
    "        #self.Y = pd.get_dummies(self.df['fs'])\n",
    "        \n",
    "        self.X_train = self.df['text'].drop(test_index)\n",
    "        self.X_train = self.X_train.sample(frac=1).reset_index(drop=True)\n",
    "        X_train_index = list(self.X_train.index)\n",
    "        self.X_test = self.df['text'].drop(X_train_index)\n",
    "        self.X_test = self.X_test.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        self.Y_train = self.Y.drop(test_index)\n",
    "        self.Y_train = self.Y_train.sample(frac=1).reset_index(drop=True)\n",
    "        self.Y_test = self.Y.drop(X_train_index)\n",
    "        self.Y_test = self.Y_test.sample(frac=1).reset_index(drop=True)\n",
    "        # tokenize x, y\n",
    "        self.X_train = self.tokenizer.texts_to_sequences(self.X_train.values)\n",
    "        #self.X = self.tokenizer.texts_to_sequences(df_text_ori)\n",
    "        self.X_train = pad_sequences(self.X_train, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "        self.X_test = self.tokenizer.texts_to_sequences(self.X_test.values)\n",
    "        #self.X = self.tokenizer.texts_to_sequences(df_text_ori)\n",
    "        self.X_test = pad_sequences(self.X_test, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        print('X Shape of data tensor:', self.X_train.shape, self.X_test.shape)\n",
    "\n",
    "        # tokenize x, y ori \n",
    "        self.X_ori = self.tokenizer.texts_to_sequences(self.X_ori.values)\n",
    "        #self.X = self.tokenizer.texts_to_sequences(df_text_ori)\n",
    "        self.X_ori = pad_sequences(self.X_ori, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "        print('X_ori Shape of data tensor:', self.X_train.shape, self.X_ori.shape)\n",
    "        self.Y_ori = pd.get_dummies(self.Y_ori).values\n",
    "        print('Shape of label tensor:', self.Y_ori.shape)\n",
    "\n",
    "        '''print('xori, yori: ', self.X_ori.shape, self.Y_ori.shape)\n",
    "        self.X_train_ori, self.X_test_ori, self.Y_train_ori, self.Y_test_ori = train_test_split(self.X_ori, self.Y_ori, test_size=0.2, random_state=42)'''\n",
    "        \n",
    "        '''self.Y_train = pd.get_dummies(self.Y_train).values\n",
    "        self.Y_test = pd.get_dummies(self.Y_test).values'''\n",
    "        print('Y Shape of label tensor:', self.Y_train.shape, self.Y_test.shape)\n",
    "\n",
    "        print('X train Shape of data tensor:', self.X_train.shape,'X test: ', self.X_test.shape)\n",
    "        print('Y Shape of label tensor:', self.Y_train.shape,'X test: ', self.Y_test.shape)\n",
    "        \n",
    "     \n",
    "    def set_model_lstm(self, topk_num = 5):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=self.X_train.shape[1]))\n",
    "        self.model.add(SpatialDropout1D(0.2))\n",
    "        self.model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "        self.model.add(Dense(37, activation='sigmoid'))\n",
    "        self.model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[tf.keras.metrics.Recall(top_k = topk_num), 'accuracy'])\n",
    "    \n",
    "    def set_model_cnn(self, topk_num = 5):\n",
    "        filter_length = 300\n",
    "        num_classes = 37\n",
    "        self.modelCNN = tf.keras.Sequential()\n",
    "        self.modelCNN.add(Embedding(MAX_NB_WORDS, 20, input_length= MAX_SEQUENCE_LENGTH))\n",
    "        self.modelCNN.add(Dropout(0.1))\n",
    "        self.modelCNN.add(tf.keras.layers.Conv1D(filter_length, 3, padding = 'valid', activation = 'relu', strides = 1))\n",
    "        self.modelCNN.add(tf.keras.layers.GlobalMaxPool1D())\n",
    "        self.modelCNN.add(Dense(num_classes))\n",
    "        self.modelCNN.add(tf.keras.layers.Activation('sigmoid'))\n",
    "        self.modelCNN.compile(optimizer = 'adam', loss  = 'binary_crossentropy', metrics = [tf.keras.metrics.Recall(top_k = topk_num)])\n",
    "\n",
    "    def run_model(self):\n",
    "        epochs = 20\n",
    "        batch_size = 64\n",
    "\n",
    "        self.history = self.model.fit(self.X_train, self.Y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    \n",
    "    def run_model_cnn(self):\n",
    "        self.modelCNN.summary()\n",
    "        self.modelCNN.fit(self.X_train, self.Y_train, epochs=15, batch_size = 64, validation_split=0.1)\n",
    "\n",
    "    def run_model_ori(self):\n",
    "        epochs = 5\n",
    "        batch_size = 64\n",
    "\n",
    "        self.history = self.model.fit(self.X_train_ori, self.Y_train_ori, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    \n",
    "    def test_model(self):\n",
    "        self.accr = self.model.evaluate(self.X_test, self.Y_test)\n",
    "        print('Test set\\n Loss: {:0.3f}\\n Accuracy: {0.3f}'.format(self.accr[0], self.accr[1]))\n",
    "\n",
    "    def test_model_ori(self):\n",
    "        self.accr = self.model.evaluate(self.X_test_ori, self.Y_test_ori)\n",
    "        print('Test set\\n Loss: {:0.3f}\\n Accuracy: {0.3f}'.format(self.accr[0], self.accr[1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 43064 entries, 0 to 43063\n",
      "Data columns (total 40 columns):\n",
      " #   Column                  Non-Null Count  Dtype \n",
      "---  ------                  --------------  ----- \n",
      " 0   text                    43064 non-null  object\n",
      " 1   labels                  43064 non-null  object\n",
      " 2   component               43064 non-null  object\n",
      " 3   auto-failover           43064 non-null  int64 \n",
      " 4   azure                   43064 non-null  int64 \n",
      " 5   benchmarks              43064 non-null  int64 \n",
      " 6   bin                     43064 non-null  int64 \n",
      " 7   build                   43064 non-null  int64 \n",
      " 8   conf                    43064 non-null  int64 \n",
      " 9   contrib/cloud           43064 non-null  int64 \n",
      " 10  contrib/eclipse-plugin  43064 non-null  int64 \n",
      " 11  contrib/hod             43064 non-null  int64 \n",
      " 12  contrib/serialization   43064 non-null  int64 \n",
      " 13  documentation           43064 non-null  int64 \n",
      " 14  filecache               43064 non-null  int64 \n",
      " 15  fs                      43064 non-null  int64 \n",
      " 16  fs/azure                43064 non-null  int64 \n",
      " 17  fs/s3                   43064 non-null  int64 \n",
      " 18  fs/swift                43064 non-null  int64 \n",
      " 19  ha                      43064 non-null  int64 \n",
      " 20  io                      43064 non-null  int64 \n",
      " 21  ipc                     43064 non-null  int64 \n",
      " 22  kms                     43064 non-null  int64 \n",
      " 23  metrics                 43064 non-null  int64 \n",
      " 24  native                  43064 non-null  int64 \n",
      " 25  net                     43064 non-null  int64 \n",
      " 26  nfs                     43064 non-null  int64 \n",
      " 27  performance             43064 non-null  int64 \n",
      " 28  record                  43064 non-null  int64 \n",
      " 29  scripts                 43064 non-null  int64 \n",
      " 30  security                43064 non-null  int64 \n",
      " 31  site                    43064 non-null  int64 \n",
      " 32  test                    43064 non-null  int64 \n",
      " 33  tools                   43064 non-null  int64 \n",
      " 34  tools/distcp            43064 non-null  int64 \n",
      " 35  tracing                 43064 non-null  int64 \n",
      " 36  trash                   43064 non-null  int64 \n",
      " 37  util                    43064 non-null  int64 \n",
      " 38  viewfs                  43064 non-null  int64 \n",
      " 39  yetus                   43064 non-null  int64 \n",
      "dtypes: int64(37), object(3)\n",
      "memory usage: 13.1+ MB\n",
      "None\n",
      "fs                      5712\n",
      "build                   4851\n",
      "security                4200\n",
      "test                    3318\n",
      "documentation           2996\n",
      "                        ... \n",
      "filecache,io,ipc           7\n",
      "filecache,fs               7\n",
      "ha,tools                   7\n",
      "conf,tools                 7\n",
      "fs,fs/swift,security       7\n",
      "Name: component, Length: 183, dtype: int64\n",
      "fs                        6300\n",
      "build                     5488\n",
      "security                  4417\n",
      "test                      3395\n",
      "documentation             3339\n",
      "ipc                       2625\n",
      "io                        2485\n",
      "conf                      2156\n",
      "scripts                   1862\n",
      "util                      1855\n",
      "fs/s3                     1624\n",
      "metrics                   1190\n",
      "native                     952\n",
      "contrib/hod                798\n",
      "yetus                      539\n",
      "tools                      497\n",
      "ha                         476\n",
      "kms                        336\n",
      "net                        301\n",
      "record                     301\n",
      "bin                        231\n",
      "contrib/cloud              224\n",
      "tools/distcp               196\n",
      "nfs                        182\n",
      "viewfs                     168\n",
      "fs/swift                   168\n",
      "auto-failover              161\n",
      "benchmarks                 147\n",
      "performance                119\n",
      "azure                      105\n",
      "contrib/eclipse-plugin      91\n",
      "filecache                   77\n",
      "site                        77\n",
      "tracing                     70\n",
      "fs/azure                    56\n",
      "trash                       42\n",
      "contrib/serialization       14\n",
      "Name: component, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "dataloc1 = \"data/hadoop/HADOOP_char_Keyboard_ori.csv\"\n",
    "pius_word1 = Lstm_issue(dataloc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataloc1 = \"data/hadoop/HADOOP_char_Keyboard_ori.csv\"\n",
    "dataloc2 = \"data/hadoop/HADOOP_char_OCR_ori.csv\"\n",
    "dataloc3 = \"data/hadoop/HADOOP_word_Antonym_ori.csv\"\n",
    "dataloc4 = \"data/hadoop/HADOOP_word_Spelling_ori.csv\"\n",
    "dataloc5 = \"data/hadoop/HADOOP_word_Split_ori.csv\"\n",
    "dataloc6 = \"data/hadoop/HADOOP_word_Synonym_ori.csv\"\n",
    "dataloc7 = \"data/hadoop/HADOOP_word_TfidfAug_ori.csv\"\n",
    "dataloc8 = \"data/hadoop/HADOOP_word_ContextualWordEmbs_ori.csv\"\n",
    "\n",
    "pius_word1 = Lstm_issue(dataloc1)\n",
    "pius_word2 = Lstm_issue(dataloc2)\n",
    "pius_word3 = Lstm_issue(dataloc3)\n",
    "pius_word4 = Lstm_issue(dataloc4)\n",
    "pius_word5 = Lstm_issue(dataloc5)\n",
    "pius_word6 = Lstm_issue(dataloc6)\n",
    "pius_word7 = Lstm_issue(dataloc7)\n",
    "pius_word8 = Lstm_issue(dataloc8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\PiusHwang\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:52: FutureWarning:\n",
      "\n",
      "The default value of regex will change from True to False in a future version.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 175288 unique tokens.\n",
      "X Shape of data tensor: (40602, 250) (2462, 250)\n",
      "X_ori Shape of data tensor: (40602, 250) (4921, 250)\n",
      "Shape of label tensor: (1231, 92)\n",
      "Y Shape of label tensor: (40602, 37) (2462, 37)\n",
      "X train Shape of data tensor: (40602, 250) X test:  (2462, 250)\n",
      "Y Shape of label tensor: (40602, 37) X test:  (2462, 37)\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 250, 20)           1000000   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 250, 20)           0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 248, 300)          18300     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 300)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 37)                11137     \n",
      "                                                                 \n",
      " activation (Activation)     (None, 37)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,029,437\n",
      "Trainable params: 1,029,437\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n"
     ]
    }
   ],
   "source": [
    "#pius_word1.print_plot(2)\n",
    "pius_word1.preprocessing()\n",
    "pius_word1.clean_text()\n",
    "pius_word1.tokenize_df()\n",
    "pius_word1.labels_to_int()\n",
    "\n",
    "word_hist1 = []\n",
    "for topk in list(range(5, 16, 5)):\n",
    "    pius_word1.set_model_cnn(topk_num=topk)\n",
    "    pius_word1.run_model_cnn()\n",
    "\n",
    "    accr = pius_word1.modelCNN.evaluate(pius_word1.X_test, pius_word1.Y_test)\n",
    "    print('Test set\\n Loss: {:0.3f}\\n Accuracy: {:0.3f}'.format(accr[0], accr[1]))\n",
    "\n",
    "    word_hist1.append(pius_word1.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "Session cannot generate requests",
     "output_type": "error",
     "traceback": [
      "Error: Session cannot generate requests",
      "at S.executeCodeCell (c:\\Users\\PiusHwang\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:301742)",
      "at S.execute (c:\\Users\\PiusHwang\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:300732)",
      "at S.start (c:\\Users\\PiusHwang\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:296408)",
      "at runMicrotasks (<anonymous>)",
      "at processTicksAndRejections (internal/process/task_queues.js:93:5)",
      "at async t.CellExecutionQueue.executeQueuedCells (c:\\Users\\PiusHwang\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:312326)",
      "at async t.CellExecutionQueue.start (c:\\Users\\PiusHwang\\.vscode\\extensions\\ms-toolsai.jupyter-2021.10.1101450599\\out\\client\\extension.js:66:311862)"
     ]
    }
   ],
   "source": [
    "word_hist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'function' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-cb086fc2d78d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0midentity\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'function' and 'float'"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2cafbbdf5f0c485b1a1935b2358d1e2de8ca6414272176d54707d0003e55811a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
